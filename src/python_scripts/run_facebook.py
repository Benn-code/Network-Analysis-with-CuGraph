# -*- coding: utf-8 -*-
"""Red_Facebook.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15v75tVemkZWWd1HUwiDlro_x5diCPWCB
"""

# Instalaci칩n de cuDF, cuML, cuGraph y dem치s usando pip
!pip install cudf-cu12 cuml-cu12 cugraph-cu12 --extra-index-url=https://pypi.nvidia.com
!pip install pyvis

#importar archivo
from google.colab import files
uploaded = files.upload()

import cugraph
import cudf

G = nx.read_edgelist("facebook_combined.txt", create_using=nx.Graph(), nodetype=int)
print("Nodos:", G.number_of_nodes())
print("Aristas:", G.number_of_edges())

# Convertir a cuGraph (de NetworkX a cudf dataframe)
edges = nx.to_pandas_edgelist(G)
edges_cudf = cudf.DataFrame.from_pandas(edges[['source', 'target']])

# Crear grafo cuGraph
G_cu = cugraph.Graph()
G_cu.from_cudf_edgelist(edges_cudf, source='source', destination='target')

#identificar como vienen los resultados
edges_df = G_cu.view_edge_list().to_pandas()
print(edges_df.columns)

#LOUVAIN
import networkx as nx
import matplotlib.pyplot as plt
#implementar louvain
parts, _ = cugraph.louvain(G_cu)

#numero de comunidades
num_comunidades = parts['partition'].nunique()
print(f"Se detectaron {num_comunidades} comunidades.")

#mapear como un diccionario para graficar
comunidades = dict(zip(parts['vertex'].to_pandas(), parts['partition'].to_pandas()))
edges_df = G_cu.view_edge_list().to_pandas()

#crear grafo NetworkX desde aristas
G_nx = nx.from_pandas_edgelist(edges_df, source='source', target='target')

#colorear nodos por comunidad
colores = [comunidades[n] for n in G_nx.nodes()]

# 6. Dibujar grafo
plt.figure(figsize=(12, 8))
pos = nx.spring_layout(G_nx, seed=42)
nx.draw(G_nx, pos, node_color=colores, cmap=plt.cm.tab20, node_size=40, with_labels=False)
plt.title("Comunidades detectadas por Louvain")
plt.show()

#PAGERANK
#ejecutar pagerank
pagerank_df = cugraph.pagerank(G_fb)

# Mostrar los 10 nodos con mayor PageRank
top_pagerank = pagerank_df.sort_values(by="pagerank", ascending=False).head(20)
print(top_pagerank)
#out: indice, ID nodo, PageRank (la suma de todos da 1)
#los indices son mucho menores que en la red mas peque침a, ser influyente en una red mas grande es mas dificil

# Calcular centralidad de grado
degree_df = cugraph.degree_centrality(G_fb)

# Mostrar los 10 nodos con mayor centralidad de grado
top_degree = degree_df.sort_values(by='degree_centrality', ascending=False).head(20)
print(top_degree)
#out: nodo, distribucion de centralidad
#el nodo 107 es el que mas amigos tiene, el 11 12 y 15 tambien tienen buen nivel

#JACCARD: busca nodos similares (util para conectar amigos)
#cargar datos
edges_fb = cudf.read_csv("facebook_combined.txt", delim_whitespace=True, names=["src", "dst"])

#crear el grafo
G_fb = cugraph.Graph()
G_fb.from_cudf_edgelist(edges_fb, source='src', destination='dst')

#implementar Jaccard
jaccard_df = cugraph.jaccard(G_fb)

#filtrar los pares iguales
jaccard_df = jaccard_df[jaccard_df['first'] != jaccard_df['second']]

#eliminar duplicados (como (1,2) y (2,1)) si el grafo es no dirigido
jaccard_df['min_node'] = jaccard_df[['first', 'second']].min(axis=1)
jaccard_df['max_node'] = jaccard_df[['first', 'second']].max(axis=1)
jaccard_df = jaccard_df.drop(columns=['first', 'second'])
jaccard_df = jaccard_df.drop_duplicates(subset=['min_node', 'max_node'])

#ordenar
jaccard_df = jaccard_df.sort_values(by='jaccard_coeff', ascending=False)

#mostrar los pares m치s similares
top_similares = jaccard_df.head(50) #head numero de pares
print(top_similares)

#out: number, coeficiente, nodo menor, nodo mayor; El nodo 11, 12 y 15 se repiten bastante